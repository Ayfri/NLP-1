# Discord NLP Models Documentation

Complete documentation of machine learning models used in `test_models.py` for advanced Discord message analysis.

## 1. Experimental Setup

| Item                    | Value |
|-------------------------|-------|
| Tasks                   | Sentiment classification + Conversation summarization |
| Data source            | Discord messages (French-dominant, mixed languages) |
| Train/test split       | 80% / 20% |
| Hardware requirement   | 4-8 GB RAM, GPU recommended |
| Expected training time | 2-10 min (GPU), 10-30 min (CPU) |
| Max samples processed  | 2000 sentiment + 200 conversations |

The script is optimized for fast training with lightweight models and automatic GPU/CPU detection.

## 2. Model Selection & Reasoning

### Sentiment Analysis Problem
Discord messages present unique challenges: they're informal, multilingual, and heavily context-dependent. Traditional rule-based approaches like lexicon matching fail completely on slang, emojis, and sarcastic expressions that are common in Discord communities.

### Summarization Problem
Long Discord conversations can span hundreds of messages across multiple topics. Users need automatic synthesis to quickly understand key discussion points without reading entire chat logs.

| Task | Model | Why This Choice |
|------|-------|----------------|
| **Sentiment** | `distilbert-base-multilingual-cased` | • Lightweight version of BERT for faster training<br>• Multilingual support handles French/English code-switching<br>• Maintains good accuracy while being 2x faster than full BERT |
| **Summarization** | `google/mt5-small` | • Multilingual T5 model optimized for French content<br>• Better handling of French conversations than English-only models<br>• Smaller size maintains training speed while improving quality |

### Library Choices

| Library | Purpose | Rationale |
|---------|---------|-----------|
| **HuggingFace Transformers** | Model loading/training | Industry standard with consistent API, extensive model zoo |
| **PyTorch** | Deep learning framework | More research-friendly than TensorFlow, better debugging tools |
| **scikit-learn** | Metrics/data splitting | Lightweight, well-tested implementations for evaluation |
| **Early Stopping** | Training optimization | Prevents overfitting and speeds up training process |

## 3. Training Configuration

### Sentiment Classification
```python
TrainingArguments(
    learning_rate=3e-5,              # Optimized for DistilBERT fine-tuning
    num_train_epochs=3,              # 3 epochs with early stopping
    per_device_train_batch_size=32,  # 32 on GPU, 16 on CPU
    per_device_eval_batch_size=64,   # Larger eval batches for speed
    eval_strategy="steps",           # Evaluate every 50 steps
    eval_steps=50,
    fp16=True,                       # Mixed precision on GPU only
    early_stopping_patience=3        # Stop if no improvement
)
```

### Conversation Summarization
```python
Seq2SeqTrainingArguments(
    learning_rate=3e-5,              # Same LR for both models
    per_device_train_batch_size=16,  # 16 on GPU, 8 on CPU
    per_device_eval_batch_size=32,   # Larger eval batches
    eval_strategy="steps",           # Evaluate every 25 steps
    eval_steps=25,
    fp16=True,                       # Mixed precision on GPU
    early_stopping_patience=2        # Faster stopping for summarization
)
```

Both configurations automatically adapt batch sizes and precision based on available hardware (GPU vs CPU).

## 4. Expected Performance

| Task | Metric | Expected Range | Notes |
|------|--------|----------------|-------|
| **Sentiment** | Accuracy | 70-80% | Performance depends heavily on label quality and data balance |
| **Summarization** | Generation Quality | Good coherence | Evaluated through manual inspection of outputs |
| **Training Speed** | GPU vs CPU | 5x faster on GPU | Significant speedup with mixed precision |

Performance metrics reflect the challenging nature of informal Discord text and the speed-accuracy tradeoff of using distilled models.

## 5. Model Architecture & Data Pipeline

### Data Sources (Generated by `main.py`)

| File | Contains | Used For |
|------|----------|----------|
| `messages_processed.csv` | Individual message analysis | Sentiment classification training |
| `conversations_analysis.csv` | Conversation-level aggregations | Conversation summarization training |

### Dataset Classes
```python
# Sentiment classification dataset (from messages_processed.csv)
DiscordMessagesDataset(max_length=128, task_type="classification")
# Uses: processed_text -> sentiment_score (3 classes: negative/neutral/positive)

# Conversation summarization dataset (from conversations_analysis.csv)
DiscordConversationDataset(max_input_length=256, max_target_length=64)
# Uses: conversation metadata -> key_topics
# Input: "Conversation between participants lasted X minutes with Y messages. Dominant emotion: Z. Sentiment: W"
# Output: "key, topics, from, conversation"
```

### Training Data Structure

**Sentiment Classification:**
- **Input**: `processed_text` (cleaned, lemmatized Discord messages)
- **Target**: `sentiment_score` converted to 3-class labels (negative < -0.1, neutral, positive > 0.1)
- **Volume**: Up to 2000 samples (automatically reduced on CPU)

**Conversation Summarization:**
- **Input**: Real French conversation content from Discord messages including:
  - `original_content` (actual message text from conversation periods)
  - Automatic cleaning of URLs, mentions, and Discord formatting
  - Intelligent truncation at sentence boundaries (max 1000 chars)
  - French language prefix to guide model understanding
- **Target**: `key_topics` formatted as "Sujets principaux: topic1, topic2, ..."
- **Volume**: Up to 200 conversations with real message content
- **Fallback**: Synthetic French descriptions when insufficient real content available

The datasets use shorter sequence lengths (128/256 tokens) for faster processing while maintaining sufficient context for Discord messages. Summarization now generates concise summaries from real French conversation content with intelligent preprocessing and multilingual model support.

### Output Structure
```
models/
├── sentiment/              # DistilBERT classification model files
├── summarization/          # DistilBART summarization model files
├── training_results.txt    # Detailed performance metrics and analysis
└── logs/                   # TensorBoard training logs for monitoring
```

## 6. Recommendation

> **Use these optimized models for fast Discord analysis** – they provide the best balance between speed and accuracy for multilingual informal text processing, with automatic GPU acceleration.

### Implementation Notes
1. **Memory**: Minimum 4GB RAM for model loading, 6GB recommended for training
2. **GPU**: Automatic detection with `fp16=True` for 40-50% faster training if CUDA available
3. **Data**: Processes up to 2000 messages for sentiment, 200 conversations for summarization
4. **Speed**: Training completes in 2-10 minutes on GPU vs 10-30 minutes on CPU

The models are specifically optimized for Discord's unique linguistic characteristics while prioritizing training speed.

## 7. Run the Models

```bash
# 1. Ensure Discord data is processed and ready
python main.py

# 2a. Train both models (auto GPU detection) - FIRST TIME
python test_models.py

# 2b. Use existing trained models (no retraining) - SUBSEQUENT RUNS
python test_models.py

# 2c. Force retrain models even if they exist
python test_models.py --retrain

# 3. Review detailed training results and metrics
start models/training_results.txt
```

### Smart Model Loading
- **First run**: Models are trained and saved to `./models/sentiment/` and `./models/summarization/`
- **Subsequent runs**: Existing models are automatically loaded (saves 2-10 minutes)
- **Force retrain**: Use `--retrain` flag to ignore existing models and train from scratch

Training automatically detects GPU availability and adjusts parameters accordingly. Progress is logged with emojis for easy monitoring.

## 8. Limitations & Alternatives

| Limitation | Impact | Alternative Solution |
|------------|--------|---------------------|
| Shorter sequences (128/256 tokens) | May truncate very long messages | Increase max_length at cost of speed |
| Distilled models | Slightly lower accuracy than full models | Use full BERT/BART if accuracy is critical |
| Limited conversation samples (200) | May not capture all conversation patterns | Increase max_samples parameter |
| GPU dependency for speed | Much slower training on CPU only | Consider cloud GPU instances for training |

Each limitation represents a speed-accuracy tradeoff that can be adjusted based on specific requirements and available computational resources.

## 9. Recent Improvements (2025)

### Summary Generation Issues Fixed
The original implementation had several critical issues that produced poor French summaries:

| **Previous Problem** | **Solution Implemented** |
|---------------------|--------------------------|
| ❌ English-only model (`distilbart-cnn-6-6`) | ✅ Multilingual model (`google/mt5-small`) |
| ❌ Synthetic English metadata as input | ✅ Real French conversation messages |
| ❌ Truncated messages (100 chars) | ✅ Smart truncation at sentence boundaries |
| ❌ No text cleaning (URLs, mentions) | ✅ Automatic cleaning with regex patterns |
| ❌ No language context | ✅ French prefix: "Résumé de conversation en français:" |

### Enhanced Text Processing Pipeline
```python
# NEW: Intelligent conversation text preparation
- Real message content extraction from time periods
- URL/mention cleaning: http://... → [lien], @user → [mention]
- Smart truncation at sentence boundaries (. ! ?)
- French language context prefix for model guidance
- Fallback quality detection and error handling
```

### Improved Generation Parameters
```python
# OPTIMIZED: Better generation settings for French
max_length=80,          # Reduced from 150 for better quality
min_length=15,          # Reduced from 20
temperature=0.7,        # Added creativity parameter
length_penalty=1.2,     # Encourage proper summary length
# Quality detection: reject poor outputs
```

These improvements should resolve the English/French mixing and incoherent outputs previously observed.
